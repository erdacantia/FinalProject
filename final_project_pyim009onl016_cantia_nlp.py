# -*- coding: utf-8 -*-
"""Final_Project_PYIM009ONL016_Cantia_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JhucD8H-ulqE5zuyhF485WeQAQlmll4b

#**Final Project - Natural Languange Processing (NLP)**

##**Pengenalan**
#### Kasus yang akan dipilih adalah Natural Languange Processing (NLP) dimana dalam hal ini akan dilakukan sentiment analysis terhadap review produk sepatu dari amazon. 
####Sumber yang digunakan : 
####https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Shoes_v1_00.tsv.gz

###Import Library
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow import keras
from tensorflow.keras import utils
from sklearn.preprocessing import LabelEncoder

import gzip
import csv
import os
import shutil
import matplotlib.pyplot as plt

# %matplotlib inline
import matplotlib as mpl

"""###Load Dataset"""

!wget https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Shoes_v1_00.tsv.gz

"""###Decompress File Dataset"""

with gzip.open('amazon_reviews_us_Shoes_v1_00.tsv.gz','rb') as f1:
  with open('amazon_reviews_us_Shoes_v1_00.tsv','wb') as f2:
    shutil.copyfileobj(f1,f2)

"""###Read Dataset"""

shoes = pd.read_csv('amazon_reviews_us_Shoes_v1_00.tsv',delimiter ='\t', error_bad_lines=False)

shoes.head(10)

shoesdata = shoes.copy()
shoesdata.head()

shoesdata.info()

"""###Cleaning Dataset"""

shoesdata.dropna(inplace=True)
shoesdata.drop_duplicates()
shoesdata.shape

"""###Splitting Dataset"""

shoesdata = shoesdata[:10000] 
datasplit = shoesdata[['review_body','star_rating']]
datasplit.head(10)

datasplit.info()

star_rating = pd.DataFrame({'rating_value':datasplit['star_rating']})
star_rating.head()

"""###Add Sentiment Type"""

star_rating.loc[star_rating.rating_value>=3,'sentiment'] = 'positive'
star_rating.loc[star_rating.rating_value<3,'sentiment'] = 'negative'
star_rating

star_rating.info()

object1=[star_rating.columns[index]for index, dtype in enumerate(star_rating.dtypes)if dtype =='object']
object1

#cek keunikan data
for column in object1:
  print(f'{column}:{star_rating[column].unique()}')

encoder = LabelEncoder()
for column in object1:
 star_rating[column]=encoder.fit_transform(star_rating[column])

star_rating.head()

dataset1 = pd.concat([datasplit,star_rating],axis=1)
dataset1.drop(['star_rating','rating_value'],axis=1,inplace=True)
dataset1.head()

train, val, test = np.split(dataset1.sample(frac=1), [int(0.8*len(dataset1)), int(0.2*len(dataset1))])

def change_to_ds(df):
  labels = df.pop('sentiment')
  dataset1 = tf.data.Dataset.from_tensor_slices((df,labels))
  return dataset1


train_dataset = change_to_ds(train)
val_dataset = change_to_ds(val)
test_dataset = change_to_ds(test)

type(train_dataset)

BUFFER_SIZE = 10000
BATCH_SIZE = 64
train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

for example, label in train_dataset.take(1):
  print('text:',example.numpy())
  print('label:',label.numpy())

VOCAB_SIZE=1000
encoder= tf.keras.layers.TextVectorization(
    max_tokens=VOCAB_SIZE)
encoder.adapt(train_dataset.map(lambda review_body, label:review_body))

encoded_ex= encoder(example)[:3].numpy()
encoded_ex

"""###Function"""

EPOCHS=8

def plot_graphs(history, metric):
  plt.plot(history.history[metric])
  plt.plot(history.history['val_'+metric],'')
  plt.xlabel('Epochs')
  plt.ylabel(metric)
  plt.legend([metric,'val'+ metric])

def loss_acc():
  test_loss, test_acc = model.evaluate(train_dataset)
  print('test loss :', test_loss)
  print('test acc :', test_acc)

def predictions(input_text):
  predictions = model.predict(np.array([input_text]))
  if predictions > 0:
    print ('Sentiment Positive')
  else:
    print('Sentiment Negative')

def figure():
  plt.figure(figsize=(16, 8))
  plt.subplot(1, 2, 1)
  plot_graphs(history, 'accuracy')
  plt.ylim(None, 1)
  plt.subplot(1, 2, 2)
  plot_graphs(history, 'loss')
  plt.ylim(0, None)

"""###Modeling"""

model = tf.keras.Sequential([
                             encoder,
                             tf.keras.layers.Embedding( 
                                 input_dim= len(encoder.get_vocabulary()),
                                 output_dim=64,
                                 mask_zero=True
                             ),
                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
                             tf.keras.layers.Dense(64, activation='relu'),
                             tf.keras.layers.Dense(1)

])

model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['accuracy'])

history = model.fit(train_dataset, epochs=EPOCHS,
                     validation_data=test_dataset,
                     validation_steps= 30,
                     verbose=1)

loss_acc()

figure()

"""##Result"""

predictions('My Sister loves the shoes')

predictions('Hurt my toes')

"""###**Kesimpulan**
####Model ini dapat mengerluarkan sentimen positif dan negatif tergantung dari review yang diberikan. 
####Apabila ada indikasi review bersifat keluhan atau negatif, maka akan terdeteksi bahwa review bersentimen negatif begitu pula sebaliknya.

##**Model Deployment**
#### Mohon maaf untuk model deployment belum bisa dilakukan karena masih belum begitu paham. Akan dipelajari lebih lanjut apabila tidak ada keterbatasan waktu. 
####Terimakasih
"""